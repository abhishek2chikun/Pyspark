{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark =SparkSession.builder.appName('practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.237:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff2ddb1ebe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "df_spark=spark.read.csv('data-20210604T074015Z-001/data/accounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+---------+\n",
      "|             _c0|    _c1|      _c2|\n",
      "+----------------+-------+---------+\n",
      "|         account|revenue|employees|\n",
      "|      Sunnamplex|4592.96|  13938.0|\n",
      "|           Silis|5339.57|  18053.0|\n",
      "|    Groovestreet|2728.86|   6486.0|\n",
      "|         Donware|2009.52|   3409.0|\n",
      "|Wonka Industries|4962.27|   4687.0|\n",
      "|        Faxquote|4939.54|  17075.0|\n",
      "|      Zathunicon|2913.82|   7332.0|\n",
      "|      Stanredtax|  14.79|     26.0|\n",
      "|          Cheers|1124.33|   1531.0|\n",
      "|     Good Burger| 247.91|    304.0|\n",
      "|      Lexiqvolax| 652.53|   1644.0|\n",
      "|         Nam-zim| 369.58|   1167.0|\n",
      "|        Ron-tech|3805.02|   7596.0|\n",
      "|         Finhigh| 398.18|   1311.0|\n",
      "|          Sonron| 588.46|   1668.0|\n",
      "|         Domzoom|2238.78|   3439.0|\n",
      "|        Gogozoom| 3577.1|  10379.0|\n",
      "|        Scotfind|3911.27|   8284.0|\n",
      "|       Ganjaflex|  161.8|    240.0|\n",
      "+----------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd,schema=['a','b','c','d','e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To view\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print schema of rdd\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a     #X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('a').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)         #Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'bigint'),\n",
       " ('b', 'double'),\n",
       " ('c', 'string'),\n",
       " ('d', 'date'),\n",
       " ('e', 'timestamp')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(a > 2)|\n",
      "+-------+\n",
      "|  false|\n",
      "|  false|\n",
      "|   true|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.a > 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|  a|                  e|\n",
      "+---+-------------------+\n",
      "|  1|2000-01-01 12:00:00|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a==1).select(['a','e']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|summary|  a|      c|\n",
      "+-------+---+-------+\n",
      "|  count|  3|      3|\n",
      "|   mean|2.0|   null|\n",
      "| stddev|1.0|   null|\n",
      "|    min|  1|string1|\n",
      "|    max|  3|string3|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('a','c').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+-------+\n",
      "|summary|  a|  b|      c|\n",
      "+-------+---+---+-------+\n",
      "|  count|  3|  3|      3|\n",
      "|   mean|2.0|3.0|   null|\n",
      "| stddev|1.0|1.0|   null|\n",
      "|    min|  1|2.0|string1|\n",
      "|    max|  3|4.0|string3|\n",
      "+-------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding and Delecting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+---+\n",
      "|  a|  b|      c|         d|                  e| aa|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|  1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|  4|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|  9|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=df.withColumn('aa',df['a']*df['a'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete\n",
    "x=x.drop('aa')\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRow=spark.createDataFrame([Row(a=4,b=1.1,c='abhishek',d='NaN',e='NaN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----------+-------------------+\n",
      "|  a|  b|       c|         d|                  e|\n",
      "+---+---+--------+----------+-------------------+\n",
      "|  1|2.0| string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0| string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0| string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|1.1|abhishek|       NaN|                NaN|\n",
      "+---+---+--------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----------+-------------------+\n",
      "|  a|  b|       c|         d|                  e|\n",
      "+---+---+--------+----------+-------------------+\n",
      "|  1|2.0| string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0| string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0| string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|1.1|abhishek|      null|               null|\n",
      "+---+---+--------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.na.replace('NaN',None)      #Replacing\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show() #Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----------+-------------------+\n",
      "|  a|  b|       c|         d|                  e|\n",
      "+---+---+--------+----------+-------------------+\n",
      "|  1|2.0| string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0| string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0| string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|1.1|abhishek|helloworld|         helloworld|\n",
      "+---+---+--------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('helloworld').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_square(series:pd.Series)->pd.Series:\n",
    "    return series*series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|pandas_square(a)|\n",
      "+----------------+\n",
      "|               1|\n",
      "|               4|\n",
      "|               9|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(pandas_square(df.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_filter_fun(iterator):\n",
    "    for i in iterator:\n",
    "        yield i[i.a == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.mapInPandas(pandas_filter_fun,schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "|black|    6.0|   60.0|\n",
      "| blue|    3.0|   30.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-------+\n",
      "|color| fruit|avg(v1)|avg(v2)|\n",
      "+-----+------+-------+-------+\n",
      "| blue| grape|    4.0|   40.0|\n",
      "|black|carrot|    6.0|   60.0|\n",
      "|  red|banana|    4.0|   40.0|\n",
      "|  red| grape|    8.0|   80.0|\n",
      "|  red|carrot|    4.0|   40.0|\n",
      "| blue|banana|    2.0|   20.0|\n",
      "+-----+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(['color','fruit']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_mean(data):\n",
    "    return data.assign(v1=(data.v2+data.v1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  5| 10|\n",
      "|  red|carrot| 16| 30|\n",
      "|  red|carrot| 27| 50|\n",
      "|  red|banana| 38| 70|\n",
      "|  red| grape| 44| 80|\n",
      "|black|carrot| 33| 60|\n",
      "| blue|banana| 11| 20|\n",
      "| blue| grape| 22| 40|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('color').applyInPandas(pandas_mean,schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('foo.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  7| 70|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|  red|banana|  1| 10|\n",
      "|  red|carrot|  3| 30|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('foo.csv',header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
